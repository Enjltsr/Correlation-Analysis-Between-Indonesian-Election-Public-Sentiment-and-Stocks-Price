{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction"
      ],
      "metadata": {
        "id": "-UY3W760kWTb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data diri\n"
      ],
      "metadata": {
        "id": "SVkWP-p2kgQ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nama : Enjelita Sari\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "NIM  :H1D020021\n",
        "\n",
        "---\n",
        "\n",
        "Judul Penelitian:\n",
        "Analisis Korelasi Sentimen Pemilu 2024 dan Pergerakan Saham Pelaku Politik di Indonesia"
      ],
      "metadata": {
        "id": "HLg-DU6skr_k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Daftar Isi"
      ],
      "metadata": {
        "id": "UAD5_yHhlFW0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[Introduction](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=-UY3W760kWTb)\n",
        "\n",
        ">>[Data diri](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=SVkWP-p2kgQ2)\n",
        "\n",
        ">>[Daftar Isi](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=UAD5_yHhlFW0)\n",
        "\n",
        ">[Data Crawling](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=xnf-JC5YaNxA)\n",
        "\n",
        ">>[Input Auth Token Twitter](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=HfaSqMWebjxU)\n",
        "\n",
        ">>[Preparing](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=HL0fAQSKbqj9)\n",
        "\n",
        ">>[Crawl Data](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=LeVr1quIb0ab)\n",
        "\n",
        ">>[Display Data Hasil Crawling](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=qyd8hTQUb2-L)\n",
        "\n",
        ">[Data Cleaning](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=WFX0A-AscE1I)\n",
        "\n",
        ">>[Import Library](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=9cxeIAxLcNv3)\n",
        "\n",
        ">>[Load Data](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=uS4efx5LcR83)\n",
        "\n",
        ">>[Hapus Kolom yang tidak diperlukan](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=l4f6Un7schO8)\n",
        "\n",
        ">>[Preprocessing](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=Qz3qsQbScts8)\n",
        "\n",
        ">>>[Cleansing](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=Sx3YAXjncx_m)\n",
        "\n",
        ">>>[Case Folding](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=vCMYeKqBdSG6)\n",
        "\n",
        ">>>[Menghapus Stopwords](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=Oxl05hkGdWUV)\n",
        "\n",
        ">>>[Tokenisasi](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=rzKYw0ardenR)\n",
        "\n",
        ">>>[Stemming](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=zN1-_HqidibN)\n",
        "\n",
        ">>>[Translate](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=6odpBBiudw0w)\n",
        "\n",
        ">>[Save Data Bersih](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=R1WRTyfad60F)\n",
        "\n",
        ">[Modelling LSTM](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=KjdbTftreH9E)\n",
        "\n",
        ">>[Import Library](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=ovHqJFv5eTTz)\n",
        "\n",
        ">>[Import Data Latih](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=xYzblJNBeVXY)\n",
        "\n",
        ">>[Membersihkan Data Latih](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=mG94ElcEeaPG)\n",
        "\n",
        ">>[Split Data](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=R9gcFT-fej2N)\n",
        "\n",
        ">>[Pra-pemrosesan Data Latih](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=dRCuN5snfWmu)\n",
        "\n",
        ">>[Set Hyperparameter](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=dFztCFpSer0l)\n",
        "\n",
        ">>[Inisialisasi Model](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=GxuCy9WGfg51)\n",
        "\n",
        ">>[Compile Model](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=DxQqX6Mtfnfn)\n",
        "\n",
        ">>[Latih Model](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=y9DW8QVwfssY)\n",
        "\n",
        ">>[Ringkasan Model](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=wF12B6_afvpV)\n",
        "\n",
        ">>[Klasifikasikan Hasil](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=nE9UEhFqf1hO)\n",
        "\n",
        ">>[Evaluasi Model pada Training set](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=iAm6ljUngIna)\n",
        "\n",
        ">>[Confusion Matrix](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=ChY63l_OgWLF)\n",
        "\n",
        ">>[Save Model](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=iOr5HUUUg2Qw)\n",
        "\n",
        ">[Implementasi Model ke Data Test Crawling](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=TOn-vlY0hGRB)\n",
        "\n",
        ">>[Load Model](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=CuUhuNdzhThY)\n",
        "\n",
        ">>[Import Data Test](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=NvsWuJFThYuu)\n",
        "\n",
        ">>[Prediksi Data Test](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=2fbuhbAZiAjp)\n",
        "\n",
        ">>[Visualisasi](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=p6eCoIGai2Nr)\n",
        "\n",
        ">>[Save Data Hasil Prediksi](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=CkIvQWcBjZj-)\n",
        "\n",
        ">[Analisis Korelasi](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=8sL-AvcSjgy5)\n",
        "\n",
        ">>[Import Library](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=fiGGACByjknv)\n",
        "\n",
        ">>[Import Data](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=aL3GB3zmjr7D)\n",
        "\n",
        ">>[Preparing Data](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=x1ed0lFjj1p2)\n",
        "\n",
        ">>[Korelasikan Data Sentimen dan Data Saham](#updateTitle=true&folderId=1kGjM2BalFYGYonE6BFz_D0A2433rGmEt&scrollTo=Bi8i0wg1kFsx)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "G7Ws1Yahmbu4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Crawling"
      ],
      "metadata": {
        "id": "xnf-JC5YaNxA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Input Auth Token Twitter"
      ],
      "metadata": {
        "id": "HfaSqMWebjxU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eq4SU79HZ57K"
      },
      "outputs": [],
      "source": [
        "twitter_auth_token = '3d8ce47eb7e90374fe5b9bfb20dfe0c6c23a5b31'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing"
      ],
      "metadata": {
        "id": "HL0fAQSKbqj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Python package\n",
        "!pip install pandas\n",
        "\n",
        "# Install Node.js\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y ca-certificates curl gnupg\n",
        "!sudo mkdir -p /etc/apt/keyrings\n",
        "!curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | sudo gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg\n",
        "\n",
        "!NODE_MAJOR=20 && echo \"deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_$NODE_MAJOR.x nodistro main\" | sudo tee /etc/apt/sources.list.d/nodesource.list\n",
        "\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install nodejs -y\n",
        "\n",
        "!node -v"
      ],
      "metadata": {
        "id": "typIwmFTbU0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crawl Data"
      ],
      "metadata": {
        "id": "LeVr1quIb0ab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'hasil_pemilu1.csv'\n",
        "search_keyword = 'hasil pemilu until:2024-05-03 since:2024-03-20 lang:id'\n",
        "limit = 500\n",
        "\n",
        "!npx -y tweet-harvest@2.6.1 -o \"{filename}\" -s \"{search_keyword}\" --tab \"LATEST\" -l {limit} --token {twitter_auth_token}"
      ],
      "metadata": {
        "id": "uqhNQeHxbWr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display Data Hasil Crawling"
      ],
      "metadata": {
        "id": "qyd8hTQUb2-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Specify the path to your CSV file\n",
        "file_path = f\"tweets-data/{filename}\"\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv(file_path, delimiter=\",\")\n",
        "\n",
        "# Display the DataFrame\n",
        "display(df)"
      ],
      "metadata": {
        "id": "GlYfrbc6b_28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning"
      ],
      "metadata": {
        "id": "WFX0A-AscE1I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Library"
      ],
      "metadata": {
        "id": "9cxeIAxLcNv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import seaborn as sns\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from google.colab import drive\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "plt.style.use('ggplot')"
      ],
      "metadata": {
        "id": "OElEuZ3LcHL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "uS4efx5LcR83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = pd.read_csv(\"/content/drive/MyDrive/Tugas Akhir/data_crawling/hasil_pemilu/hasil_pemilu1.csv\")\n",
        "data2 = pd.read_csv(\"/content/drive/MyDrive/Tugas Akhir/data_crawling/hasil_pemilu/hasil_pemilu2.csv\")\n",
        "data3 = pd.read_csv(\"/content/drive/MyDrive/Tugas Akhir/data_crawling/hasil_pemilu/hasil_pemilu3.csv\")\n",
        "data4 = pd.read_csv(\"/content/drive/MyDrive/Tugas Akhir/data_crawling/hasil_pemilu/hasil_pemilu4.csv\")\n",
        "data5 = pd.read_csv(\"/content/drive/MyDrive/Tugas Akhir/data_crawling/hasil_pemilu/hasil_pemilu5.csv\")\n",
        "df = pd.concat([data1, data2, data3, data4, data5], ignore_index=True)"
      ],
      "metadata": {
        "id": "CmbVK1pvcVfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hapus Kolom yang tidak diperlukan"
      ],
      "metadata": {
        "id": "l4f6Un7schO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop([\"username\",\"conversation_id_str\", \"created_at\", \"favorite_count\", \"id_str\",\n",
        "           \"image_url\", \"in_reply_to_screen_name\", \"location\", \"quote_count\",\n",
        "           \"reply_count\", \"retweet_count\", \"tweet_url\", \"user_id_str\"], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "ZiqfHLDlcl-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "Qz3qsQbScts8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import string"
      ],
      "metadata": {
        "id": "CGHrde5GcwWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleansing"
      ],
      "metadata": {
        "id": "Sx3YAXjncx_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_pattern(text, pattern_regex):\n",
        "  r = re.findall(pattern_regex, text)\n",
        "  for i in r:\n",
        "    text = re.sub(i, '', text)\n",
        "  return text\n",
        "\n",
        "df['clean_tweet']=np.vectorize(remove_pattern)(df['full_text'], \" *RT* | *@[\\w]* \")\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "M1eY4OC-cztV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove(text):\n",
        "  text = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\s+)\",\" \",text).split())\n",
        "  return text\n",
        "df['remove_http'] = df['clean_tweet'].apply(lambda x: remove(x))\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "gCSdh7Z7c139"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove(tweet):\n",
        "  #remove stock market tickers\n",
        "  tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "\n",
        "  #remove old style retweet\n",
        "  tweet= re.sub(r'^RT[\\s]+', '', tweet)\n",
        "\n",
        "  #remove hashtags\n",
        "  tweet = re.sub(r'#', '', tweet)\n",
        "\n",
        "  #remove angka\n",
        "  tweet = re.sub('[0-9]+', '', tweet)\n",
        "\n",
        "  return tweet\n",
        "df['cleaned'] = df['remove_http'].apply(lambda x: remove(x))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "rsYExSQkc4tX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop_duplicates(subset=\"cleaned\", keep= 'first', inplace = True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "oY5kCtDtc5cJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Case Folding"
      ],
      "metadata": {
        "id": "vCMYeKqBdSG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def low(text):\n",
        "  text = text.lower()\n",
        "  return text\n",
        "\n",
        "df['lower'] = df['cleaned'].str.lower()\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "vTuT5y7fdUmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Menghapus Stopwords"
      ],
      "metadata": {
        "id": "Oxl05hkGdWUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Menggabungkan stopwords NLTK dengan stopwords kustom\n",
        "with open('/content/drive/MyDrive/Tugas Akhir/stopword.txt', 'r', encoding='utf-8') as file:\n",
        "    custom_stopwords = file.read().splitlines()\n",
        "\n",
        "# Menggabungkan stopwords NLTK dengan stopwords kustom\n",
        "all_stopwords = set(stopwords.words('indonesian')).union(set(custom_stopwords))\n",
        "\n",
        "# Fungsi untuk menghapus stopwords dari teks\n",
        "def remove_stopwords(text, stop_words):\n",
        "    # Memecah teks menjadi kata-kata\n",
        "    words = text.split()\n",
        "    # Menghapus stopwords\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    # Menggabungkan kembali kata-kata yang tersisa menjadi sebuah kalimat\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "\n",
        "# Menghilangkan stopwords dari kolom 'full_text_stemmed' di DataFrame\n",
        "df['stopword'] = df['lower'].apply(lambda x: remove_stopwords(x, all_stopwords))\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "kt2fx5XddY4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenisasi"
      ],
      "metadata": {
        "id": "rzKYw0ardenR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    exclude = set(string.punctuation + string.digits)\n",
        "    text = ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    # Split the text into individual words\n",
        "    tokens = re.findall(r'\\w+', text)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "df['tokenize'] = df['stopword'].apply(tokenize)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "8CebQulzdhbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming"
      ],
      "metadata": {
        "id": "zN1-_HqidibN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Fact = StemmerFactory()\n",
        "Stemmer = Fact.create_stemmer()\n",
        "\n",
        "def stem(text):\n",
        "  return Stemmer.stem(text)  # Apply stemming to the word\n",
        "\n",
        "# Assuming \"full_text_tokenize\" is the column containing tokenized text\n",
        "df[\"stemmed\"] = df[\"tokenize\"].apply(lambda tokens: [stem(token) for token in tokens])\n",
        "df"
      ],
      "metadata": {
        "id": "ShhCS9I9dgPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Translate"
      ],
      "metadata": {
        "id": "6odpBBiudw0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install googletrans==4.0.0-rc1\n",
        "\n",
        "def untokenize(tokens):\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Menerapkan fungsi untokenize pada kolom 'tokenized_text'\n",
        "df['clean'] = df['stemmed'].apply(untokenize)\n",
        "\n",
        "import googletrans\n",
        "from googletrans import Translator\n",
        "translator = Translator()\n",
        "\n",
        "def translate_text(text):\n",
        "    try:\n",
        "        # Menerjemahkan teks dari bahasa Inggris ke bahasa Indonesia\n",
        "        translated = translator.translate(text, src='id', dest='en')\n",
        "        return translated.text\n",
        "    except Exception as e:\n",
        "        return str(e)\n",
        "\n",
        "\n",
        "df['translate'] = df['clean'].apply(translate_text)\n",
        "df"
      ],
      "metadata": {
        "id": "wOyAJyUZdywb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Data Bersih"
      ],
      "metadata": {
        "id": "R1WRTyfad60F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"data_gerindra_bersih.csv\", index=False)\n",
        "print(\"DataFrame saved\")"
      ],
      "metadata": {
        "id": "qRvgpUq9d96G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelling LSTM"
      ],
      "metadata": {
        "id": "KjdbTftreH9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Library"
      ],
      "metadata": {
        "id": "ovHqJFv5eTTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "import math\n",
        "import nltk\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "IHIOc4s3eMK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Data Latih"
      ],
      "metadata": {
        "id": "xYzblJNBeVXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = pd.read_csv('/content/drive/MyDrive/Tugas Akhir/data_train/Prabowo Subianto.csv')\n",
        "data2 = pd.read_csv('/content/drive/MyDrive/Tugas Akhir/data_train/Ganjar Pranowo.csv')\n",
        "data3 = pd.read_csv('/content/drive/MyDrive/Tugas Akhir/data_train/Anies Baswedan.csv')\n",
        "\n",
        "data = pd.concat([data1, data2, data3], ignore_index=True)\n",
        "data = data.dropna()"
      ],
      "metadata": {
        "id": "onfMpTTWeXzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Membersihkan Data Latih"
      ],
      "metadata": {
        "id": "mG94ElcEeaPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Read custom stopwords\n",
        "with open('/content/drive/MyDrive/Tugas Akhir/code/stopword.txt', 'r', encoding='utf-8') as file:\n",
        "    custom_stopwords = file.read().splitlines()\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english')).union(set(custom_stopwords))\n",
        "\n",
        "    # Pastikan input adalah string\n",
        "    if isinstance(text, str):\n",
        "        word_tokens = word_tokenize(text)\n",
        "        filtered_text = [word for word in word_tokens if word not in stop_words]\n",
        "        return ' '.join(filtered_text)\n",
        "    else:\n",
        "        return text  # Kembalikan input asli jika bukan string\n",
        "\n",
        "# Terapkan fungsi ke kolom 'Text'\n",
        "data['Text'] = data['Text'].apply(lambda x: remove_stopwords(x))\n",
        "data"
      ],
      "metadata": {
        "id": "fIAnQR2neiOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split Data\n"
      ],
      "metadata": {
        "id": "R9gcFT-fej2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = data['Text'].values\n",
        "labels = data['label'].values\n",
        "encoder = LabelEncoder()\n",
        "encoded_labels = encoder.fit_transform(labels)\n",
        "\n",
        "train_sentences, test_sentences, train_labels, test_labels = train_test_split(reviews, encoded_labels, stratify = encoded_labels)"
      ],
      "metadata": {
        "id": "H6mrLcLDeoa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pra-pemrosesan Data Latih"
      ],
      "metadata": {
        "id": "dRCuN5snfWmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure all entries in train_sentences are strings\n",
        "train_sentences = [str(sentence) for sentence in train_sentences]\n",
        "\n",
        "# tokenize sentences\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# convert train dataset to sequences and pad sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "train_padded = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length)\n",
        "\n",
        "# Ensure all entries in test_sentences are strings\n",
        "test_sentences = [str(sentence) for sentence in test_sentences]\n",
        "\n",
        "# convert test dataset to sequences and pad sequences\n",
        "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "test_padded = pad_sequences(test_sequences, padding=padding_type, maxlen=max_length)\n"
      ],
      "metadata": {
        "id": "flw054WKeygF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Hyperparameter"
      ],
      "metadata": {
        "id": "dFztCFpSer0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 15000\n",
        "oov_tok = '<OOV>'\n",
        "embedding_dim = 64\n",
        "max_length = 150\n",
        "padding_type = 'post'\n",
        "trunc_type = 'post'"
      ],
      "metadata": {
        "id": "041MB-K9euMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inisialisasi Model"
      ],
      "metadata": {
        "id": "GxuCy9WGfg51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Input(shape=(max_length,)),\n",
        "    keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "    keras.layers.Bidirectional(keras.layers.LSTM(64)),\n",
        "    keras.layers.Dense(24, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')"
      ],
      "metadata": {
        "id": "iiJguHx3flH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compile Model"
      ],
      "metadata": {
        "id": "DxQqX6Mtfnfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Ube2T7tEfj85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Latih Model"
      ],
      "metadata": {
        "id": "y9DW8QVwfssY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "history = model.fit(train_padded, train_labels,\n",
        "                    epochs=num_epochs, verbose=1,\n",
        "                    validation_split=0.1)"
      ],
      "metadata": {
        "id": "-vHO5Nd1fu3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ringkasan Model"
      ],
      "metadata": {
        "id": "wF12B6_afvpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "tBYym2ZmfziL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Klasifikasikan Hasil"
      ],
      "metadata": {
        "id": "nE9UEhFqf1hO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = model.predict(test_padded)\n",
        "pred_labels = []\n",
        "for i in prediction:\n",
        "    if i >= 0.5:\n",
        "        pred_labels.append(1)\n",
        "    else:\n",
        "        pred_labels.append(0)"
      ],
      "metadata": {
        "id": "AXPXd-ANgG-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluasi Model pada Training set"
      ],
      "metadata": {
        "id": "iAm6ljUngIna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_predictions = (model.predict(X_train) > 0.5).astype(\"int32\")\n",
        "train_accuracy = accuracy_score(y_train, train_predictions)\n",
        "train_precision = precision_score(y_train, train_predictions)\n",
        "train_recall = recall_score(y_train, train_predictions)\n",
        "\n",
        "print(f\"Training Accuracy: {train_accuracy:.2f}\")\n",
        "print(f\"Training Precision: {train_precision:.2f}\")\n",
        "print(f\"Training Recall: {train_recall:.2f}\")\n",
        "\n",
        "# Evaluasi model pada validation set\n",
        "val_predictions = (model.predict(X_val) > 0.5).astype(\"int32\")\n",
        "val_accuracy = accuracy_score(y_val, val_predictions)\n",
        "val_precision = precision_score(y_val, val_predictions)\n",
        "val_recall = recall_score(y_val, val_predictions)\n",
        "\n",
        "print(f\"Validation Accuracy: {val_accuracy:.2f}\")\n",
        "print(f\"Validation Precision: {val_precision:.2f}\")\n",
        "print(f\"Validation Recall: {val_recall:.2f}\")"
      ],
      "metadata": {
        "id": "xC4n-7URgK4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrix"
      ],
      "metadata": {
        "id": "ChY63l_OgWLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hitung confusion matrix\n",
        "conf_matrix = confusion_matrix(test_labels, pred_labels)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OrURKq52gZRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghitung metrik evaluasi\n",
        "accuracy = accuracy_score(test_labels, pred_labels)\n",
        "precision = precision_score(test_labels, pred_labels, pos_label=1, average='binary')\n",
        "recall = recall_score(test_labels, pred_labels, pos_label=1, average='binary')\n",
        "class_report = classification_report(test_labels, pred_labels, target_names=['Negative', 'Positive'])\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"Classification Report:\\n\", class_report)"
      ],
      "metadata": {
        "id": "5DJsRCX6gt34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Model"
      ],
      "metadata": {
        "id": "iOr5HUUUg2Qw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('lstm.h5')"
      ],
      "metadata": {
        "id": "aiXRq6avg5TS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementasi Model ke Data Test Crawling"
      ],
      "metadata": {
        "id": "TOn-vlY0hGRB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model"
      ],
      "metadata": {
        "id": "CuUhuNdzhThY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "model = load_model('/content/drive/MyDrive/Tugas Akhir/code/lstm.h5')"
      ],
      "metadata": {
        "id": "4DU9fMbohNgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Data Test"
      ],
      "metadata": {
        "id": "NvsWuJFThYuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = pd.read_csv('/content/drive/MyDrive/Tugas Akhir/data_clean/data_perindo_bersih.csv')\n",
        "data2 = pd.read_csv('/content/drive/MyDrive/Tugas Akhir/data_clean/data_gerindra_bersih.csv')\n",
        "data3 = pd.read_csv('/content/drive/MyDrive/Tugas Akhir/data_clean/data_pdip_bersih.csv')\n",
        "data4 = pd.read_csv('/content/drive/MyDrive/Tugas Akhir/data_clean/data_psi_bersih.csv')\n",
        "data5 = pd.read_csv('/content/drive/MyDrive/Tugas Akhir/data_clean/data_golkar_bersih.csv')\n",
        "data6 = pd.read_csv('/content/drive/MyDrive/Tugas Akhir/data_clean/data_ppp_bersih.csv')\n",
        "data7 = pd.read_csv('/content/drive/MyDrive/Tugas Akhir/data_clean/data_hasil_pemilu_bersih.csv')\n",
        "test_df = pd.concat([data1, data2, data3, data4, data5, data6, data7])"
      ],
      "metadata": {
        "id": "4lGzYy3Dhb_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Baca data test dari file CSV\n",
        "test = test_df['clean']\n",
        "\n",
        "# Ambil kolom 'text' sebagai daftar kalimat\n",
        "test_sentences = test.tolist()"
      ],
      "metadata": {
        "id": "1axvSZE1hcty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediksi Data Test"
      ],
      "metadata": {
        "id": "2fbuhbAZiAjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Parameter yang sama seperti saat pelatihan\n",
        "vocab_size = 3000\n",
        "oov_tok = ''\n",
        "max_length = 200\n",
        "padding_type = 'post'\n",
        "trunc_type = 'post'\n",
        "\n",
        "test_sentences = [str(sentence) for sentence in test_sentences if isinstance(sentence, (str, int, float))]\n",
        "\n",
        "# Tokenisasi data test\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(test_sentences)  # Perlu fit_on_texts agar tokenizer mengetahui kata-kata\n",
        "\n",
        "# Konversi test dataset ke sequences dan padding\n",
        "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "test_padded = pad_sequences(test_sequences, padding=padding_type, maxlen=max_length)\n",
        "\n",
        "# Prediksi pada data test\n",
        "predictions = model.predict(test_padded)\n",
        "\n",
        "# Tentukan threshold untuk masing-masing kelas\n",
        "positive_threshold = 0.7\n",
        "negative_threshold = 0.3\n",
        "\n",
        "# Ubah hasil prediksi menjadi label \"positive\", \"neutral\", dan \"negative\"\n",
        "def classify(prob):\n",
        "    if prob >= positive_threshold:\n",
        "        return 'positive'\n",
        "    elif prob < negative_threshold:\n",
        "        return 'negative'\n",
        "    else:\n",
        "        return 'neutral'\n",
        "\n",
        "# Terapkan fungsi classify ke hasil prediksi\n",
        "predicted_labels = [classify(prob) for prob in predictions.flatten()]\n",
        "\n",
        "# Tambahkan hasil prediksi ke DataFrame asli\n",
        "test_df['predicted_label'] = predicted_labels\n",
        "\n",
        "# Tampilkan beberapa hasil untuk verifikasi\n",
        "print(test_df[['clean', 'predicted_label']].head())"
      ],
      "metadata": {
        "id": "gCRXbrtaiC74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualisasi"
      ],
      "metadata": {
        "id": "p6eCoIGai2Nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualisasi distribusi prediksi\n",
        "plt.hist(predictions.flatten(), bins=10, edgecolor='black')\n",
        "plt.xlabel('Predicted Probability')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Predicted Probabilities')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2X3pip8KiPsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Menghitung jumlah prediksi untuk setiap kategori\n",
        "positive_count = sum(1 for label in predicted_labels if label == 'positive')\n",
        "neutral_count = sum(1 for label in predicted_labels if label == 'neutral')\n",
        "negative_count = sum(1 for label in predicted_labels if label == 'negative')\n",
        "\n",
        "# Data untuk plot\n",
        "labels = ['Positive', 'Neutral', 'Negative']\n",
        "counts = [positive_count, neutral_count, negative_count]\n",
        "\n",
        "# Plot histogram\n",
        "plt.figure(figsize=(8, 6))\n",
        "bars = plt.bar(labels, counts, color=['green', 'blue', 'red'])\n",
        "plt.xlabel('Sentiment Category')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Predicted Sentiment')\n",
        "plt.ylim(0, len(predicted_labels))  # Menyesuaikan skala sumbu y dengan jumlah total prediksi\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Tambahkan label pada bar\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.3, round(yval, 2), ha='center', va='bottom')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p1l1Y8v_jQ3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Data Hasil Prediksi"
      ],
      "metadata": {
        "id": "CkIvQWcBjZj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.to_csv(\"data_sentimen_hasil_pemilu.csv\", index=False)"
      ],
      "metadata": {
        "id": "YOFEeh46jekb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analisis Korelasi"
      ],
      "metadata": {
        "id": "8sL-AvcSjgy5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Library"
      ],
      "metadata": {
        "id": "fiGGACByjknv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import spearmanr\n",
        "from datetime import datetime\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Qgeewde8jp3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Data"
      ],
      "metadata": {
        "id": "aL3GB3zmjr7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_sentimen = pd.read_csv('/content/drive/MyDrive/Tugas Akhir/data_sentimen_hasil_pemilu.csv')\n",
        "data_saham = pd.read_csv('/content/drive/MyDrive/Tugas Akhir/Data saham fix.csv')"
      ],
      "metadata": {
        "id": "V63BjK5Wjvw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Data"
      ],
      "metadata": {
        "id": "x1ed0lFjj1p2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_sentimen = data_sentimen.drop(columns=['full_text', 'translate', 'clean_tweet', 'lower', 'stopword', 'tokenize', 'stemmed', 'clean'])\n",
        "data_sentimen.rename(columns={'created_at': 'Tanggal'}, inplace=True)\n",
        "\n",
        "# Map kategori sentimen ke angka\n",
        "map_sentimen = {'positive': 1, 'neutral': 0, 'negative': -1}\n",
        "data_sentimen['predicted_label'] = data_sentimen['predicted_label'].map(map_sentimen)"
      ],
      "metadata": {
        "id": "uqjQHv8yjyvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi normalisasi saham\n",
        "def normalisasi_saham(nilai):\n",
        "    if nilai < 10:  # Jika nilai di bawah 10, dianggap nilai desimal dan dikalikan 1000\n",
        "        return nilai * 1000\n",
        "    else:  # Jika nilai di atas 10, biarkan nilai tetap\n",
        "        return nilai\n",
        "\n",
        "# Konversi kolom 'Terakhir' ke tipe numerik\n",
        "data_saham['Terakhir'] = pd.to_numeric(data_saham['Terakhir'], errors='coerce')\n",
        "\n",
        "# Terapkan fungsi normalisasi\n",
        "data_saham['Terakhir'] = data_saham['Terakhir'].apply(normalisasi_saham)"
      ],
      "metadata": {
        "id": "8MyTuFTVj6X1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk mengubah format tanggal\n",
        "def ubah_format_tanggal(tanggal_str):\n",
        "    # Parsing string tanggal ke datetime object\n",
        "    tanggal_dt = datetime.strptime(tanggal_str, \"%a %b %d %H:%M:%S %z %Y\")\n",
        "    # Mengubah datetime object ke string dengan format yang diinginkan\n",
        "    return tanggal_dt.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# Mengubah format tanggal pada kolom created_at\n",
        "data_sentimen['Tanggal'] = data_sentimen['Tanggal'].apply(ubah_format_tanggal)\n",
        "data_sentimen['Tanggal'] = pd.to_datetime(data_sentimen['Tanggal'])"
      ],
      "metadata": {
        "id": "uazZhRuOj-tL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hasil_sentimen = data_sentimen.groupby('Tanggal').agg({'predicted_label': 'sum'})\n",
        "hasil_saham = data_saham.groupby('Tanggal').agg({'Terakhir': 'mean'})"
      ],
      "metadata": {
        "id": "owUIULPlkBq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_length = min(len(hasil_sentimen), len(hasil_saham))\n",
        "hasil_sentimen = hasil_sentimen[:min_length]\n",
        "hasil_saham = hasil_saham[:min_length]"
      ],
      "metadata": {
        "id": "BLGzTQlKkLRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Korelasikan Data Sentimen dan Data Saham"
      ],
      "metadata": {
        "id": "Bi8i0wg1kFsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghitung korelasi Spearman\n",
        "correlation, p_value = spearmanr(hasil_sentimen, hasil_saham)\n",
        "\n",
        "print(\"Korelasi Spearman:\", correlation)\n",
        "print(\"Nilai p:\", p_value)"
      ],
      "metadata": {
        "id": "afwH2kWukJ2H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}